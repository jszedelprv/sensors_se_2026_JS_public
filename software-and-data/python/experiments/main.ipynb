{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83210df7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "personal_dataset_path './datasets/sensors-2026-freehand/as-captured/apsz' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experiments, AUGM_RATIO, IMAGE_SIZE, DROP_RATE, N_LEA, DATA_PART, N_VAL\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmethods\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_samples_and_labels, make_stratified_splits, make_lookup_table, load_and_preprocess_image, make_metric_data_frame, flatten_metric_data_frame\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_augmentation\n",
      "File \u001b[1;32mc:\\__VWork\\edu\\badania\\kai\\_gd_kai_062025\\_gd_kai_062025\\python\\experiments\\experiments.py:28\u001b[0m\n\u001b[0;32m      8\u001b[0m DROP_RATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m\n\u001b[0;32m      9\u001b[0m DATA_PART \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas-captured\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m experiments \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m     make_experiment(\n\u001b[0;32m     13\u001b[0m         personal_dataset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msensors-2026-freehand\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m DATA_PART \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjasz\u001b[39m\u001b[38;5;124m\"\u001b[39m, casual_gestures_dataset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msensors-2026-freehand\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m DATA_PART \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcash\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflowers_model jasz\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     14\u001b[0m         dropout_rate\u001b[38;5;241m=\u001b[39mDROP_RATE, input_shape\u001b[38;5;241m=\u001b[39m(IMAGE_SIZE, IMAGE_SIZE, \u001b[38;5;241m1\u001b[39m), model_gen\u001b[38;5;241m=\u001b[39mflowers_model, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, samples_per_class_train\u001b[38;5;241m=\u001b[39mN_LEA, samples_per_class_val\u001b[38;5;241m=\u001b[39mN_VAL, rotation_factor\u001b[38;5;241m=\u001b[39mAUGM_RATIO, translation_factor\u001b[38;5;241m=\u001b[39mAUGM_RATIO, zoom_factor\u001b[38;5;241m=\u001b[39mAUGM_RATIO, repetitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     15\u001b[0m     ),\n\u001b[0;32m     16\u001b[0m     make_experiment(\n\u001b[0;32m     17\u001b[0m         personal_dataset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msensors-2026-freehand\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m DATA_PART \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mszsz\u001b[39m\u001b[38;5;124m\"\u001b[39m, casual_gestures_dataset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msensors-2026-freehand\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m DATA_PART \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcash\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflowers_model szsz\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     18\u001b[0m         dropout_rate\u001b[38;5;241m=\u001b[39mDROP_RATE, input_shape\u001b[38;5;241m=\u001b[39m(IMAGE_SIZE, IMAGE_SIZE, \u001b[38;5;241m1\u001b[39m), model_gen\u001b[38;5;241m=\u001b[39mflowers_model, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, samples_per_class_train\u001b[38;5;241m=\u001b[39mN_LEA, samples_per_class_val\u001b[38;5;241m=\u001b[39mN_VAL, rotation_factor\u001b[38;5;241m=\u001b[39mAUGM_RATIO, translation_factor\u001b[38;5;241m=\u001b[39mAUGM_RATIO, zoom_factor\u001b[38;5;241m=\u001b[39mAUGM_RATIO, repetitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     19\u001b[0m     ),\n\u001b[0;32m     20\u001b[0m     make_experiment(\n\u001b[0;32m     21\u001b[0m         personal_dataset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msensors-2026-freehand\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m DATA_PART \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkals\u001b[39m\u001b[38;5;124m\"\u001b[39m, casual_gestures_dataset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msensors-2026-freehand\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m DATA_PART \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcash\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflowers_model kals\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     22\u001b[0m         dropout_rate\u001b[38;5;241m=\u001b[39mDROP_RATE, input_shape\u001b[38;5;241m=\u001b[39m(IMAGE_SIZE, IMAGE_SIZE, \u001b[38;5;241m1\u001b[39m), model_gen\u001b[38;5;241m=\u001b[39mflowers_model, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, samples_per_class_train\u001b[38;5;241m=\u001b[39mN_LEA, samples_per_class_val\u001b[38;5;241m=\u001b[39mN_VAL, rotation_factor\u001b[38;5;241m=\u001b[39mAUGM_RATIO, translation_factor\u001b[38;5;241m=\u001b[39mAUGM_RATIO, zoom_factor\u001b[38;5;241m=\u001b[39mAUGM_RATIO, repetitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     23\u001b[0m     ),\n\u001b[0;32m     24\u001b[0m     make_experiment(\n\u001b[0;32m     25\u001b[0m         personal_dataset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msensors-2026-freehand\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m DATA_PART \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwosz\u001b[39m\u001b[38;5;124m\"\u001b[39m, casual_gestures_dataset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msensors-2026-freehand\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m DATA_PART \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcash\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflowers_model wosz\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     26\u001b[0m         dropout_rate\u001b[38;5;241m=\u001b[39mDROP_RATE, input_shape\u001b[38;5;241m=\u001b[39m(IMAGE_SIZE, IMAGE_SIZE, \u001b[38;5;241m1\u001b[39m), model_gen\u001b[38;5;241m=\u001b[39mflowers_model, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, samples_per_class_train\u001b[38;5;241m=\u001b[39mN_LEA, samples_per_class_val\u001b[38;5;241m=\u001b[39mN_VAL, rotation_factor\u001b[38;5;241m=\u001b[39mAUGM_RATIO, translation_factor\u001b[38;5;241m=\u001b[39mAUGM_RATIO, zoom_factor\u001b[38;5;241m=\u001b[39mAUGM_RATIO, repetitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     27\u001b[0m     ),\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mmake_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersonal_dataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msensors-2026-freehand\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDATA_PART\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapsz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasual_gestures_dataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msensors-2026-freehand\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDATA_PART\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflowers_model apsz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDROP_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_gen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflowers_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples_per_class_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_LEA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples_per_class_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_VAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotation_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAUGM_RATIO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslation_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAUGM_RATIO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzoom_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAUGM_RATIO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     32\u001b[0m ]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# experiments = [\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# make_experiment(\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m#     personal_dataset_path=r\"datasets\\sensors-2026-freehand\\as-captured\\jasz\", casual_gestures_dataset_path=r\"datasets\\sensors-2026-freehand\\as-captured\\cash\", name='flowers_model jasz', \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# ),\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\__VWork\\edu\\badania\\kai\\_gd_kai_062025\\_gd_kai_062025\\python\\experiments\\methods.py:32\u001b[0m, in \u001b[0;36mmake_experiment\u001b[1;34m(personal_dataset_path, casual_gestures_dataset_path, name, batch_size, epochs, learning_rate, dropout_rate, samples_per_class_train, samples_per_class_val, input_shape, rotation_factor, zoom_factor, translation_factor, model_gen, repetitions)\u001b[0m\n\u001b[0;32m     12\u001b[0m personal_dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m personal_dataset_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m experiment \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpersonal_dataset_path\u001b[39m\u001b[38;5;124m'\u001b[39m: personal_dataset_path,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasual_gestures_dataset_path\u001b[39m\u001b[38;5;124m'\u001b[39m: casual_gestures_dataset_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepetitions\u001b[39m\u001b[38;5;124m'\u001b[39m: repetitions\n\u001b[0;32m     30\u001b[0m }\n\u001b[1;32m---> 32\u001b[0m \u001b[43mcheck_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m experiment\n",
      "File \u001b[1;32mc:\\__VWork\\edu\\badania\\kai\\_gd_kai_062025\\_gd_kai_062025\\python\\experiments\\methods.py:50\u001b[0m, in \u001b[0;36mcheck_experiment\u001b[1;34m(experiment)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(experiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_factor\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m experiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_factor\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslation_factor must be a float between 0 and 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(experiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_gen\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_gen must be a callable function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(experiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpersonal_dataset_path\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersonal_dataset_path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpersonal_dataset_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(experiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasual_gestures_dataset_path\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcasual_gestures_dataset_path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasual_gestures_dataset_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(experiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m experiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate must be a positive float\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: personal_dataset_path './datasets/sensors-2026-freehand/as-captured/apsz' does not exist"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt, image as mpimg\n",
    "from matplotlib.gridspec import GridSpecFromSubplotSpec \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from experiments import experiments, AUGM_RATIO, IMAGE_SIZE, DROP_RATE, N_LEA, DATA_PART, N_VAL\n",
    "from methods import get_samples_and_labels, make_stratified_splits, make_lookup_table, load_and_preprocess_image, make_metric_data_frame, flatten_metric_data_frame\n",
    "from models import make_augmentation\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import seaborn as sns\n",
    "from seaborn import heatmap\n",
    "from typing import List, Dict\n",
    "\n",
    "print(\"number of training samples = \", N_LEA)\n",
    "print(\"number of validation samples = \", N_VAL)\n",
    "print(\"image size = \", IMAGE_SIZE)\n",
    "print(\"augmentation factor = \", AUGM_RATIO)\n",
    "print(\"dropout rate = \", DROP_RATE)\n",
    "print(\"dataset partition = \", DATA_PART)\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46008814",
   "metadata": {},
   "source": [
    "# Global environment initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e451b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = []\n",
    "NUM_CLASSES = []\n",
    "NUM_SAMPLES = []\n",
    "NUM_SAMPLES_PER_CLASS = []\n",
    "SAMPLES_PER_CLASS_TRAIN = []\n",
    "SAMPLES_PER_CLASS_VAL = []\n",
    "SAMPLES_PER_CLASS_TEST = []\n",
    "TRAIN_SIZE = [] \n",
    "VAL_SIZE = []\n",
    "TEST_SIZE = []\n",
    "CLASS_TO_IDX_TABLE: List[tf.lookup.StaticHashTable] = []\n",
    "IDX_TO_CLASS_TABLE: List[tf.lookup.StaticHashTable] = []\n",
    "train_datasets = []\n",
    "val_datasets = []\n",
    "test_datasets = []\n",
    "casual_gestures_datasets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7805a0",
   "metadata": {},
   "source": [
    "# DO NOT RUN MULTIPLE TIMES WITHOUT RESET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiments:\n",
    "    X, y = get_samples_and_labels(experiment['personal_dataset_path'])\n",
    "    X_cas, y_cas = get_samples_and_labels(experiment['casual_gestures_dataset_path'])\n",
    "    X_cas = list(X_cas)\n",
    "    y_cas = list(y_cas)\n",
    "\n",
    "    CLASS_NAMES.append(sorted(set(y)))\n",
    "    NUM_CLASSES.append(len(CLASS_NAMES[-1]))\n",
    "    NUM_SAMPLES.append(len(X))\n",
    "    NUM_SAMPLES_PER_CLASS.append(NUM_SAMPLES[-1] // NUM_CLASSES[-1])\n",
    "    SAMPLES_PER_CLASS_TRAIN.append(experiment['samples_per_class_train'])\n",
    "    SAMPLES_PER_CLASS_VAL.append(experiment['samples_per_class_val'])\n",
    "    SAMPLES_PER_CLASS_TEST.append(NUM_SAMPLES_PER_CLASS[-1] - SAMPLES_PER_CLASS_TRAIN[-1] - SAMPLES_PER_CLASS_VAL[-1])\n",
    "    TRAIN_SIZE.append(SAMPLES_PER_CLASS_TRAIN[-1] * NUM_CLASSES[-1])\n",
    "    VAL_SIZE.append(SAMPLES_PER_CLASS_VAL[-1] * NUM_CLASSES[-1])\n",
    "    TEST_SIZE.append(NUM_SAMPLES[-1] - TRAIN_SIZE[-1] - VAL_SIZE[-1])\n",
    "    keys_to_vals, vals_to_keys = make_lookup_table(CLASS_NAMES[-1])\n",
    "    CLASS_TO_IDX_TABLE.append(keys_to_vals)\n",
    "    IDX_TO_CLASS_TABLE.append(vals_to_keys)\n",
    "\n",
    "    assert TRAIN_SIZE[-1] + VAL_SIZE[-1] <= NUM_SAMPLES[-1], \"Not enough samples for the specified train/val split.\"\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = make_stratified_splits(X, y, VAL_SIZE[-1], TEST_SIZE[-1])\n",
    "    train_datasets.append(train_dataset)\n",
    "    val_datasets.append(val_dataset)\n",
    "    test_datasets.append(test_dataset)\n",
    "    casual_gestures_datasets.append(tf.data.Dataset.from_tensor_slices((X_cas, y_cas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eb6c3b",
   "metadata": {},
   "source": [
    "# Models initialization - run following block to reset trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd98ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "models: List[tf.keras.Sequential] = []\n",
    "histories = []\n",
    "test_results = []\n",
    "y_pred_pobs = []\n",
    "y_preds = []\n",
    "y_csh_pred_pobs = []\n",
    "y_csh_preds = []\n",
    "y_true_classes = []\n",
    "y_csh_true_classes_one_hot = []\n",
    "y_true_classes_one_hot = []\n",
    "for i, experiment in enumerate(experiments):\n",
    "    models.append(experiment['model_gen'](experiment['input_shape'], NUM_CLASSES[i], experiment['learning_rate'], experiment['dropout_rate']))\n",
    "    histories.append([])\n",
    "    y_preds.append([])\n",
    "    test_results.append([])\n",
    "    y_pred_pobs.append([])\n",
    "    y_csh_preds.append([])\n",
    "    y_csh_pred_pobs.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af202c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, experiment in enumerate(experiments):\n",
    "    print(f\"Starting experiment {i+1}/{len(experiments)}: {experiment['name']}\")\n",
    "    models[i].summary()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11c67a",
   "metadata": {},
   "source": [
    "# Training loop - run following block to conduct the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb2bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, experiment in enumerate(experiments):\n",
    "    if TEST_SIZE[i] > 0:\n",
    "        test_dataset = test_datasets[i].map(lambda path, label: (load_and_preprocess_image(path, label, CLASS_TO_IDX_TABLE[i], experiment['input_shape'], NUM_CLASSES[i])))\n",
    "        casual_gestures_dataset = casual_gestures_datasets[i].map(lambda path, label: (load_and_preprocess_image(path, label, CLASS_TO_IDX_TABLE[i], experiment['input_shape'], NUM_CLASSES[i])))\n",
    "        casual_gestures_dataset = casual_gestures_dataset.shuffle(1000).take(TEST_SIZE[i])\n",
    "        casual_gestures_dataset = test_dataset.concatenate(casual_gestures_dataset)\n",
    "        y_true_classes.append(tf.argmax([y for _, y in test_dataset], axis=1))\n",
    "        y_true_classes_one_hot.append([y for _, y in test_dataset])\n",
    "        y_csh_true_classes_one_hot.append([y for _, y in casual_gestures_dataset])\n",
    "        test_dataset = test_dataset.batch(experiment['batch_size']).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        casual_gestures_dataset = casual_gestures_dataset.batch(experiment['batch_size']).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    for j in range(experiment['repetitions']):\n",
    "        print(f\"Starting experiment {i+1}/{len(experiments)}: {experiment['name']} repetition: {j+1}/{experiment['repetitions']}\")\n",
    "\n",
    "        model = tf.keras.models.clone_model(models[i])\n",
    "        model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "        augmentation = make_augmentation(experiment['rotation_factor'], experiment['zoom_factor'], experiment['translation_factor'])\n",
    "\n",
    "        train_dataset = train_datasets[i].map(lambda path, label: (load_and_preprocess_image(path, label, CLASS_TO_IDX_TABLE[i], experiment['input_shape'], NUM_CLASSES[i])))\n",
    "        train_dataset = train_dataset.batch(experiment['batch_size']).shuffle(reshuffle_each_iteration=True, buffer_size=train_dataset.cardinality()).cache().map(\n",
    "            lambda x, y: (augmentation(x), y) , num_parallel_calls=tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE\n",
    "        )\n",
    "\n",
    "        val_dataset = val_datasets[i].map(lambda path, label: (load_and_preprocess_image(path, label, CLASS_TO_IDX_TABLE[i], experiment['input_shape'], NUM_CLASSES[i])))\n",
    "        val_dataset = val_dataset.batch(experiment['batch_size']).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        histories[i].append(\n",
    "            model.fit(\n",
    "                train_dataset,\n",
    "                validation_data=val_dataset,\n",
    "                epochs=experiment['epochs'],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if TEST_SIZE[i] > 0:\n",
    "            restults = model.evaluate(test_dataset)\n",
    "            test_results[i].append(restults)\n",
    "            y_pred_pobs[i].append(model.predict(test_dataset))\n",
    "            y_pred =tf.argmax(y_pred_pobs[i][-1], axis=1)\n",
    "            y_preds[i].append(y_pred)\n",
    "            y_csh_pred_pobs[i].append(model.predict(casual_gestures_dataset))\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16298a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_y_csh_true_classes_one_hot = np.array(y_csh_true_classes_one_hot)\n",
    "np_y_csh_pred_pobs = []\n",
    "for i in range(len(experiments)):\n",
    "    np_y_csh_pred_pobs.append(np.array(y_csh_pred_pobs[i]))\n",
    "\n",
    "tpr_all_experiments = np.zeros((len(experiments), 100))\n",
    "roc_auc_all_experiments = np.zeros(len(experiments))\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "for i, experiment_histories in enumerate(histories):\n",
    "    #EXPERIMENT INFO\n",
    "    loss = make_metric_data_frame(experiments[i], experiment_histories, 'loss')\n",
    "    val_loss = make_metric_data_frame(experiments[i], experiment_histories, 'val_loss')\n",
    "    accuracy = make_metric_data_frame(experiments[i], experiment_histories, 'accuracy')\n",
    "    val_accuracy = make_metric_data_frame(experiments[i], experiment_histories, 'val_accuracy')\n",
    "    epochs = experiments[i]['epochs']\n",
    "    repetitions = experiments[i]['repetitions']\n",
    "\n",
    "    trainig_average_minimum_loss = np.average([np.min(loss['loss'][j:(j + 1)*epochs]) for j in range(repetitions)])\n",
    "    validation_average_minimum_loss = np.average([np.min(val_loss['val_loss'][j:(j + 1)*epochs]) for j in range(repetitions)])\n",
    "    trainig_average_maximum_accuracy = np.average([np.max(accuracy['accuracy'][j:(j + 1)*epochs]) for j in range(repetitions)])\n",
    "    validation_average_maximum_accuracy = np.average([np.max(val_accuracy['val_accuracy'][j:(j + 1)*epochs]) for j in range(repetitions)])\n",
    "\n",
    "    trainig_minimum_average_loss = np.min(loss.groupby(['epoch']).mean()['loss'])\n",
    "    validation_minimum_average_loss = np.min(val_loss.groupby(['epoch']).mean()['val_loss'])\n",
    "    trainig_maximum_average_accuracy = np.max(accuracy.groupby(['epoch']).mean()['accuracy'])\n",
    "    validation_maximum_average_accuracy = np.max(val_accuracy.groupby(['epoch']).mean()['val_accuracy'])\n",
    "\n",
    "    print(f\"Experiment {i+1}: {experiments[i]['name']} | Dataset Path: {experiments[i]['personal_dataset_path']}\")\n",
    "    print()\n",
    "    print(f\"Class Names: {CLASS_NAMES[i]}\")\n",
    "    print(f\"Number of Classes: {NUM_CLASSES[i]} |  Total Samples: {NUM_SAMPLES[i]} | Samples per Class: {NUM_SAMPLES_PER_CLASS[i]}\")\n",
    "    print(f\"Training Samples: {TRAIN_SIZE[i]} ({SAMPLES_PER_CLASS_TRAIN[i]} per class) | Validation Samples: {VAL_SIZE[i]} ({SAMPLES_PER_CLASS_VAL[i]} per class) | Testing Samples: {TEST_SIZE[i]} ({SAMPLES_PER_CLASS_TEST[i]} per class)\")\n",
    "    print(f\"Epochs: {experiments[i]['epochs']} | Repetitions: {experiments[i]['repetitions']} | Learning Rate: {experiments[i]['learning_rate']} | Input shape: {experiments[i]['input_shape']}\")\n",
    "    print(f\"Dropout Rate: {experiments[i]['dropout_rate']} | Rotation Factor: {experiments[i]['rotation_factor']} | Zoom Factor: {experiments[i]['zoom_factor']} | Translation Factor: {experiments[i]['translation_factor']}\")\n",
    "    print()\n",
    "    print(\"Training: average minimum loss:\", trainig_average_minimum_loss, \"| average maximum accuracy:\", trainig_average_maximum_accuracy)\n",
    "    print(\"Validation: average minimum loss:\", validation_average_minimum_loss, \"| average maximum accuracy:\", validation_average_maximum_accuracy)\n",
    "    print()\n",
    "    print(\"Training: minimum average loss:\", trainig_minimum_average_loss, \"| maximum average accuracy:\", trainig_maximum_average_accuracy)\n",
    "    print(\"Validation: minimum average loss:\", validation_minimum_average_loss, \"| maximum average accuracy:\", validation_maximum_average_accuracy)\n",
    "    print()\n",
    "    print(\"Test results: average loss\", np.average([result[0] for result in test_results[i]]), \"| average accuracy:\", np.average([result[1] for result in test_results[i]]) if TEST_SIZE[i] > 0 else \"No test set\")\n",
    "\n",
    "    #LOSS AND ACC PLOTS\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1).minorticks_on()\n",
    "    sns.lineplot(data=loss, x='epoch', y='loss', label='training loss', linestyle=':')\n",
    "    sns.lineplot(data=val_loss, x='epoch', y='val_loss', label='validation loss', color='red', linestyle='--')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.axis(ymin=0, ymax=4, xmin=1, xmax=epochs)\n",
    "\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    ax.set_yticks([tick / 10 for tick in range(11)])\n",
    "    ax.minorticks_on()\n",
    "    sns.lineplot(data=accuracy, x='epoch', y='accuracy', label='training accuracy', linestyle=':')\n",
    "    sns.lineplot(data=val_accuracy, x='epoch', y='val_accuracy', label='validation accuracy', color='red', linestyle='--')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.axis(ymin=0, ymax=1, xmin=1, xmax=epochs)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    #ROC PLOTS\n",
    "    tpr = np.zeros((repetitions, NUM_CLASSES[i],100))\n",
    "    roc_auc = np.zeros((repetitions, NUM_CLASSES[i])) \n",
    "\n",
    "    for repetition in range(repetitions):\n",
    "        for n in range(NUM_CLASSES[i]):\n",
    "            fpr_tmp, tpr_tmp, _ = roc_curve(np_y_csh_true_classes_one_hot[i, :, n], np_y_csh_pred_pobs[i][repetition, :, n])\n",
    "            tpr[repetition, n] = np.interp(mean_fpr, fpr_tmp, tpr_tmp)\n",
    "            roc_auc[repetition, n] = auc(fpr_tmp, tpr_tmp)\n",
    "            tpr[repetition, n, 0] = 0.0\n",
    "\n",
    "    tpr_all_reps = np.mean(tpr, axis=1)\n",
    "    roc_auc_all_reps = np.mean(roc_auc, axis=1)\n",
    "\n",
    "    tpr_experiment = np.mean(tpr_all_reps, axis=0)\n",
    "    roc_auc_experiment = np.mean(roc_auc_all_reps, axis=0)\n",
    "\n",
    "    tpr_all_experiments[i] = tpr_experiment\n",
    "    roc_auc_all_experiments[i] = roc_auc_experiment\n",
    "\n",
    "    plt.figure()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.minorticks_on()\n",
    "    plt.title(\"ROC\")\n",
    "    for repetition in range(repetitions):\n",
    "        plt.plot(mean_fpr, tpr_all_reps[repetition], color=\"GREY\", alpha=0.8, lw=0.8)\n",
    "    plt.plot(mean_fpr, tpr_experiment, color=\"BLUE\", label=f\"macro-averge OvR (AUC = {roc_auc_experiment:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color=\"BLACK\", label=f\"Chance level (AUC = 0.5)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print()\n",
    "\n",
    "tpr_all_experiments_averged = np.mean(tpr_all_experiments, axis=0)\n",
    "roc_auc_all_experiments_averged = np.mean(roc_auc_all_experiments, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc115996",
   "metadata": {},
   "source": [
    "# Aggregated loss and accuracy plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e7a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "every_experiment_loss = []\n",
    "every_experiment_val_loss = []\n",
    "every_experiment_accuracy = []\n",
    "every_experiment_val_accuracy = []\n",
    "tests_loss_averages = []\n",
    "tests_accuracy_averages = []\n",
    "number_of_experiments = len(experiments)\n",
    "\n",
    "for i, experiment_histories in enumerate(histories):\n",
    "    every_experiment_loss.append(make_metric_data_frame(experiments[i], experiment_histories, 'loss'))\n",
    "    every_experiment_val_loss.append(make_metric_data_frame(experiments[i], experiment_histories, 'val_loss'))\n",
    "    every_experiment_accuracy.append(make_metric_data_frame(experiments[i], experiment_histories, 'accuracy'))\n",
    "    every_experiment_val_accuracy.append(make_metric_data_frame(experiments[i], experiment_histories, 'val_accuracy'))\n",
    "\n",
    "    if TEST_SIZE[i] > 0:\n",
    "        tests_loss_averages.append(np.average([result[0] for result in test_results[i]]))\n",
    "        tests_accuracy_averages.append(np.average([result[1] for result in test_results[i]]))\n",
    "\n",
    "\n",
    "every_experiment_loss = flatten_metric_data_frame(every_experiment_loss, 'loss', number_of_experiments)\n",
    "every_experiment_val_loss = flatten_metric_data_frame(every_experiment_val_loss, 'val_loss', number_of_experiments)\n",
    "every_experiment_accuracy = flatten_metric_data_frame(every_experiment_accuracy, 'accuracy', number_of_experiments)\n",
    "every_experiment_val_accuracy = flatten_metric_data_frame(every_experiment_val_accuracy, 'val_accuracy', number_of_experiments)\n",
    "average_testes_loss = np.average(tests_loss_averages)\n",
    "average_testes_accuracy = np.average(tests_accuracy_averages)\n",
    "\n",
    "trainig_minimum_average_loss = np.min(every_experiment_loss.groupby(['epoch']).mean()['loss'])\n",
    "validation_minimum_average_loss = np.min(every_experiment_val_loss.groupby(['epoch']).mean()['val_loss'])\n",
    "trainig_maximum_average_accuracy = np.max(every_experiment_accuracy.groupby(['epoch']).mean()['accuracy'])\n",
    "validation_maximum_average_accuracy = np.max(every_experiment_val_accuracy.groupby(['epoch']).mean()['val_accuracy'])\n",
    "\n",
    "print(\"Training: minimum average loss:\", trainig_minimum_average_loss, \"| maximum average accuracy:\", trainig_maximum_average_accuracy)\n",
    "print(\"Validation: minimum average loss:\", validation_minimum_average_loss, \"| maximum average accuracy:\", validation_maximum_average_accuracy)\n",
    "print(\"Test results: average loss\", average_testes_loss, \"| average accuracy:\", average_testes_accuracy if TEST_SIZE[i] > 0 else \"No test set\")\n",
    "print()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1).minorticks_on()\n",
    "sns.lineplot(data=every_experiment_loss, x='epoch', y='loss', label='training loss', linestyle=':')\n",
    "sns.lineplot(data=every_experiment_val_loss, x='epoch', y='val_loss', label='validation loss', color='red', linestyle='--')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.axis(ymin=0, ymax=4, xmin=1, xmax=epochs)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.set_yticks([tick / 10 for tick in range(11)])\n",
    "ax.minorticks_on()\n",
    "sns.lineplot(data=every_experiment_accuracy, x='epoch', y='accuracy', label='training accuracy', linestyle=':')\n",
    "sns.lineplot(data=every_experiment_val_accuracy, x='epoch', y='val_accuracy', label='validation accuracy', color='red', linestyle='--')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Value')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.axis(ymin=0, ymax=1, xmin=1, xmax=epochs)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.minorticks_on()\n",
    "plt.title(\"ROC\")\n",
    "for experiment in range(len(experiments)):\n",
    "    plt.plot(mean_fpr, tpr_all_experiments[experiment], color=\"GREY\", alpha=0.8, lw=0.8)\n",
    "plt.plot(mean_fpr, tpr_all_experiments_averged, color=\"BLUE\", label=f\"macro-averge OvR (AUC = {roc_auc_all_experiments_averged:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color=\"BLACK\", label=f\"Chance level (AUC = 0.5)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92525e18",
   "metadata": {},
   "source": [
    "# Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, experiment in enumerate(experiments):\n",
    "    representatives = {y.numpy().decode('utf-8'): x.numpy().decode('utf-8') for x, y in test_datasets[i]}\n",
    "    combined_confusion_matrix = np.zeros((NUM_CLASSES[i], NUM_CLASSES[i]), dtype=np.int16)\n",
    "    for repetition in range(experiment['repetitions']):\n",
    "        combined_confusion_matrix += confusion_matrix(y_true_classes[i].numpy(), y_preds[i][repetition].numpy())\n",
    "    labels = list(map(lambda x: x.decode('utf-8'), IDX_TO_CLASS_TABLE[i].lookup(tf.constant([j for j in range(NUM_CLASSES[i])], dtype=tf.int64)).numpy()))\n",
    "    images = [mpimg.imread(representatives[label]) for label in labels]\n",
    "\n",
    "    diagonal = combined_confusion_matrix.diagonal()\n",
    "    trues = diagonal.sum()\n",
    "    all = combined_confusion_matrix.sum()\n",
    "    falses = all - trues\n",
    "    accuracy = trues / all\n",
    "    precision = np.mean([diagonal[j] / combined_confusion_matrix[:, j].sum() for j in range(NUM_CLASSES[i])])\n",
    "    recall = np.mean([diagonal[j] / combined_confusion_matrix[j].sum() for j in range(NUM_CLASSES[i])])\n",
    "\n",
    "    print(f'Accuracy: {accuracy} | Precision: {precision} | Recall: {recall}')\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 7))\n",
    "    main_grid = fig.add_gridspec(1, 2)\n",
    "    heatmap_axis = fig.add_subplot(main_grid[0, 0])\n",
    "\n",
    "    hm = heatmap(combined_confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    hm.set_ylabel('True Label', fontdict={'fontsize':14}, labelpad=12)\n",
    "    hm.set_xlabel('Predicted Label', fontdict={'fontsize':14}, labelpad=12)\n",
    "    heatmap_axis.set_title('Confusion matrix')\n",
    "    heatmap_axis.set_xlabel('Predicted Label')\n",
    "    heatmap_axis.set_ylabel('True Label')\n",
    "    heatmap_axis.set_xticklabels(labels, rotation=45)\n",
    "\n",
    "    images_container_axis = fig.add_subplot(main_grid[0, 1])\n",
    "    images_container_axis.axis('off')\n",
    "    images_container_axis.set_title('Class representatives', pad=20)\n",
    "    images_grid = GridSpecFromSubplotSpec(3, 5, subplot_spec=main_grid[0, 1])\n",
    "    for j in range(len(labels)):\n",
    "        image_axis = fig.add_subplot(images_grid[j])\n",
    "        image_axis.imshow(images[j])\n",
    "        image_axis.set_title(labels[j], fontsize=8)\n",
    "        image_axis.axis('off')\n",
    "\n",
    "    fig.suptitle(f'Experiment {i+1}: {experiment[\"name\"]}', fontsize=20, y=1.02)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c3b57",
   "metadata": {},
   "source": [
    "# Following block is used to save choosen experiments metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a37da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_to_save = [] # Indices of experiments to save metrics for\n",
    "\n",
    "for experiment in experiments_to_save:\n",
    "    experiment_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime())\n",
    "    name = experiments[experiment].get('name', None)\n",
    "    if name != None:\n",
    "        name = f\"{experiments[experiment]['name']}_{experiment_time}\"\n",
    "    else:\n",
    "        name = f\"{experiment_time}\"\n",
    "    \n",
    "    os.makedirs(f\"./metrics/{name}\", exist_ok=True)\n",
    "    np.savetxt(f\"./metrics/{name}/loss.csv\", histories[experiment].history['loss'], delimiter=\",\")\n",
    "    np.savetxt(f\"./metrics/{name}/val_loss.csv\", histories[experiment].history['val_loss'], delimiter=\",\")\n",
    "    np.savetxt(f\"./metrics/{name}/accuracy.csv\", histories[experiment].history['accuracy'], delimiter=\",\")\n",
    "    np.savetxt(f\"./metrics/{name}/val_accuracy.csv\", histories[experiment].history['val_accuracy'], delimiter=\",\")\n",
    "    if TEST_SIZE[experiment] > 0:\n",
    "        np.savetxt(f\"./metrics/{name}/test_results.csv\", test_results[experiment], delimiter=\",\")\n",
    "    \n",
    "    time.sleep(1)  # Ensure unique timestamps for directory names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-se-01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
